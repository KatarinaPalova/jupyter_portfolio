{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e34b31",
   "metadata": {},
   "source": [
    "# ðŸ’¸ Analyzing Crunchbase Data on funding rounds\n",
    "\n",
    "This is a dataquest project focusing on practicing optimisation of dataset size by using appropriate datatypes in pandas and analyzing this data with an approach that combines SQL and Pandas.\n",
    "\n",
    "The dataset we will be looking at comes from Crunchbase and it contains crowdsources information on the fundraising rounds of many startups. Our dataset contains data up to the year of 2013.\n",
    "\n",
    "During this project we will:\n",
    "- Perform a short data discovery\n",
    "- Memory usage optimization\n",
    "- Loading and analysis of the data using SQL and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a363c6b",
   "metadata": {},
   "source": [
    "## Data discovery\n",
    "\n",
    "In this first step, we want to look at a sample of our data to become familiar with its contents and possible limitations.\n",
    "\n",
    "Imposed constraint for this project is a memory limit of 10MB so we will need to make sure the dataset/chunks won't exceed it. Our ultimate goal is to reduce the dataset size below 10MB so that we can work with it as a whole, however this is only possible because we are working with a pretty small dataset. In production settings this would not happen that often.\n",
    "\n",
    "The aim of this part is to:\n",
    "- Determine which columns contain redundant information and can be omitted\n",
    "- Determine chunk size that won't exceed 50% of our memory limits = 5MB\n",
    "- Get the starting memory footprint once loaded as a dataframe\n",
    "- Get the datatypes of the various columns\n",
    "- Get the amount of NaN and unique values in each column\n",
    "- Iterate on point one and see whether more columns could be excluded\n",
    "\n",
    "Let's first examine a sample of the data to see whether we need all of the columns for the analysis as well to determine the size of chunks for reading of the dataset. Our memory limit is 10MB, therefore we are looking for chunks of around 5MB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d500750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     company_permalink company_name company_category_code  \\\n",
      "0    /company/advercar     AdverCar           advertising   \n",
      "1  /company/launchgram   LaunchGram                  news   \n",
      "2        /company/utap         uTaP             messaging   \n",
      "3    /company/zoopshop     ZoopShop              software   \n",
      "4    /company/efuneral     eFuneral                   web   \n",
      "\n",
      "  company_country_code company_state_code         company_region  \\\n",
      "0                  USA                 CA                 SF Bay   \n",
      "1                  USA                 CA                 SF Bay   \n",
      "2                  USA                NaN  United States - Other   \n",
      "3                  USA                 OH               Columbus   \n",
      "4                  USA                 OH              Cleveland   \n",
      "\n",
      "    company_city          investor_permalink      investor_name  \\\n",
      "0  San Francisco  /company/1-800-flowers-com  1-800-FLOWERS.COM   \n",
      "1  Mountain View        /company/10xelerator        10Xelerator   \n",
      "2            NaN        /company/10xelerator        10Xelerator   \n",
      "3       columbus        /company/10xelerator        10Xelerator   \n",
      "4      Cleveland        /company/10xelerator        10Xelerator   \n",
      "\n",
      "  investor_category_code investor_country_code investor_state_code  \\\n",
      "0                    NaN                   USA                  NY   \n",
      "1                finance                   USA                  OH   \n",
      "2                finance                   USA                  OH   \n",
      "3                finance                   USA                  OH   \n",
      "4                finance                   USA                  OH   \n",
      "\n",
      "  investor_region investor_city funding_round_type   funded_at funded_month  \\\n",
      "0        New York      New York           series-a  2012-10-30      2012-10   \n",
      "1        Columbus      Columbus              other  2012-01-23      2012-01   \n",
      "2        Columbus      Columbus              other  2012-01-01      2012-01   \n",
      "3        Columbus      Columbus              angel  2012-02-15      2012-02   \n",
      "4        Columbus      Columbus              other  2011-09-08      2011-09   \n",
      "\n",
      "  funded_quarter  funded_year  raised_amount_usd  \n",
      "0        2012-Q4         2012            2000000  \n",
      "1        2012-Q1         2012              20000  \n",
      "2        2012-Q1         2012              20000  \n",
      "3        2012-Q1         2012              20000  \n",
      "4        2011-Q3         2011              20000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.read_csv(\"crunchbase-investments.csv\", nrows = 5, encoding = \"ISO-8859-1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf12584",
   "metadata": {},
   "source": [
    "It looks like the company_permalink, invetor_permalink, funded_month, funded_quarter and funded_year columns contain redundant information so let's drop those columns from our analysis.\n",
    "\n",
    "Now, let's determine the chunksize that appears to be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5ce3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.448305130004883\n",
      "4.990866661071777\n"
     ]
    }
   ],
   "source": [
    "columns_to_use = [\"company_name\", \"company_category_code\", \"company_country_code\", \"company_state_code\", \"company_region\", \"company_city\", \"investor_name\", \"investor_category_code\", \"investor_country_code\", \"investor_state_code\", \"investor_region\", \"investor_city\", \"funding_round_type\", \"funded_at\", \"raised_amount_usd\" ]\n",
    "print(pd.read_csv(\"crunchbase-investments.csv\", nrows = 4000, encoding = \"ISO-8859-1\").memory_usage(deep = True).sum()/(1024*1024))\n",
    "print(pd.read_csv(\"crunchbase-investments.csv\", nrows = 6000, encoding = \"ISO-8859-1\", usecols = columns_to_use).memory_usage(deep = True).sum()/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af585f7b",
   "metadata": {},
   "source": [
    "So we can see that already, just by removing the redundant columns we have been able to use larger chunks of data which will speed up all of the operations we will do. Some of these columns, especially the date ones might be beneficial if you perform a lot of operations on them but otherwise you can get the information from the date column alone. Ideally you would be using a date table within a database and join these two tables together.\n",
    "\n",
    "Now that we have a chunksize that seems appropriate, let's check that we won't surpass our 5MB limit accross the whole dataset. To do this, we will make a function to check the dataset's memory usage per chunk and per whole dataset as we'll repeat this calculation multiple times during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b5f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the name of the csv you want to size in the form a string as \"filename.csv\", \n",
    "# the chunksize you have determined as appropriate previously, the encoding of the file, a list of columns \n",
    "# you want to keep, if left empty all columns will be read, a dictionary of datatypes for each column and \n",
    "# finally the columns that should be parsed as dates. Check the pandas.read_csv documentation for examples \n",
    "# of inputs. Finally there is an option to either display the size of each chunk or just the sum, in MB. \n",
    "# No return value.\n",
    "\n",
    "def get_memory_usage(csv, size_of_chunk, file_encoding, columns = [], detail = False, data_types = {}, to_date = []):\n",
    "    import pandas as pd\n",
    "    chunk_iter = pd.read_csv(csv, chunksize = size_of_chunk, encoding = file_encoding, usecols = columns, dtype = data_types, parse_dates = to_date)\n",
    "    \n",
    "    used_memory = []\n",
    "    total_memory = 0\n",
    "    \n",
    "    for chunk in chunk_iter:\n",
    "        chunk_memory = chunk.memory_usage(deep = True).sum()/(1024*1024)\n",
    "        used_memory.append(chunk_memory)\n",
    "        total_memory += chunk_memory\n",
    "    \n",
    "    if detail:\n",
    "        i = 1\n",
    "        for chunk in used_memory:\n",
    "            print(i, \" : \", chunk)\n",
    "            i += 1\n",
    "    print(\"Total memory used:\", total_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f49b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  4.990866661071777\n",
      "2  :  4.851469993591309\n",
      "3  :  4.850440979003906\n",
      "4  :  4.835270881652832\n",
      "5  :  4.867792129516602\n",
      "6  :  4.846621513366699\n",
      "7  :  4.840594291687012\n",
      "8  :  4.478250503540039\n",
      "9  :  3.1795654296875\n",
      "Total memory used: 41.740872383117676\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage(\"crunchbase-investments.csv\", 6000, \"ISO-8859-1\", columns = columns_to_use, detail = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d0e93",
   "metadata": {},
   "source": [
    "It looks like 6000 is an appropriate chunksize as all of the chunks are under 5MB. We have also learned that our dataset is split into 9 chunks and all together the size of the created dataframe is almost 42MB.\n",
    "\n",
    "Now let's dig deeper into the columns. We will get the data types of each column as well as the size that it occupies so that we can determine which columns have wrong data types and which should be our focus if we want to reduce the memory footprint of our dataframe.\n",
    "\n",
    "It is possible that datatypes will vary between chunks which can point us to some uncleaned data so we will list all detected datatypes accross chunks for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b91dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_name : \n",
      " Size =  3.426084518432617 MB \n",
      " Data types =  [dtype('O')]\n",
      "company_category_code : \n",
      " Size =  3.2637481689453125 MB \n",
      " Data types =  [dtype('O')]\n",
      "company_country_code : \n",
      " Size =  3.0263519287109375 MB \n",
      " Data types =  [dtype('O')]\n",
      "company_state_code : \n",
      " Size =  2.963290214538574 MB \n",
      " Data types =  [dtype('O')]\n",
      "company_region : \n",
      " Size =  3.254631996154785 MB \n",
      " Data types =  [dtype('O')]\n",
      "company_city : \n",
      " Size =  3.344602584838867 MB \n",
      " Data types =  [dtype('O')]\n",
      "investor_name : \n",
      " Size =  3.7353992462158203 MB \n",
      " Data types =  [dtype('O')]\n",
      "investor_category_code : \n",
      " Size =  0.6176071166992188 MB \n",
      " Data types =  [dtype('O'), dtype('float64')]\n",
      "investor_country_code : \n",
      " Size =  2.5944480895996094 MB \n",
      " Data types =  [dtype('O'), dtype('float64')]\n",
      "investor_state_code : \n",
      " Size =  2.4316701889038086 MB \n",
      " Data types =  [dtype('O'), dtype('float64')]\n",
      "investor_region : \n",
      " Size =  3.24007511138916 MB \n",
      " Data types =  [dtype('O')]\n",
      "investor_city : \n",
      " Size =  2.821223258972168 MB \n",
      " Data types =  [dtype('O'), dtype('float64')]\n",
      "funding_round_type : \n",
      " Size =  3.2538328170776367 MB \n",
      " Data types =  [dtype('O')]\n",
      "funded_at : \n",
      " Size =  3.3792200088500977 MB \n",
      " Data types =  [dtype('O')]\n",
      "raised_amount_usd : \n",
      " Size =  0.4044952392578125 MB \n",
      " Data types =  [dtype('float64')]\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv(\"crunchbase-investments.csv\", chunksize = 6000, encoding = \"ISO-8859-1\", usecols = columns_to_use)\n",
    "\n",
    "column_sizes = {}\n",
    "column_dtypes = {}\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    columns = chunk.columns\n",
    "    for column in chunk:\n",
    "        column_type = chunk[column].dtype\n",
    "        column_memory = chunk[column].memory_usage(deep = True)/(1024*1024)\n",
    "        if column in column_sizes:\n",
    "            column_sizes[column] += column_memory\n",
    "            if column_type not in column_dtypes[column]:\n",
    "                column_dtypes[column].append(column_type)\n",
    "        else:\n",
    "            column_sizes[column] = column_memory\n",
    "            column_dtypes[column] = [column_type]\n",
    "            \n",
    "for column in column_sizes:\n",
    "    print(column, \": \\n Size = \", column_sizes[column], \"MB \\n Data types = \", column_dtypes[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402d658",
   "metadata": {},
   "source": [
    "We can see that there are multiple columns that have multiple data types, since it is the case for columns that based on column name should be strings(object), the float type detected in some chunks is probably due to NaN value. We will check that later. We can also see that the biggest columns in terms of size that we should try to shrink should be string columns, our option here is to convert them to category datatypes, but this will work only for columns with a reasonable amount of unique values.\n",
    "\n",
    "So let's make a function that will help us:\n",
    "a) confirm our suspicion about NaN values by showing the number and share of NaN values per column\n",
    "b) determine which columns contain a manageable number of unique values and could be converted to category data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0bb50474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_name \n",
      " NA values: 1 \n",
      " % of empty: 0.0018914318138831094 \n",
      " unique values: 11573\n",
      "company_category_code \n",
      " NA values: 643 \n",
      " % of empty: 1.2161906563268394 \n",
      " unique values: 43\n",
      "company_country_code \n",
      " NA values: 1 \n",
      " % of empty: 0.0018914318138831094 \n",
      " unique values: 2\n",
      "company_state_code \n",
      " NA values: 492 \n",
      " % of empty: 0.9305844524304898 \n",
      " unique values: 50\n",
      "company_region \n",
      " NA values: 1 \n",
      " % of empty: 0.0018914318138831094 \n",
      " unique values: 546\n",
      "company_city \n",
      " NA values: 533 \n",
      " % of empty: 1.0081331567996974 \n",
      " unique values: 1229\n",
      "investor_name \n",
      " NA values: 2 \n",
      " % of empty: 0.003782863627766219 \n",
      " unique values: 10465\n",
      "investor_category_code \n",
      " NA values: 50427 \n",
      " % of empty: 95.37923207868356 \n",
      " unique values: 33\n",
      "investor_country_code \n",
      " NA values: 12001 \n",
      " % of empty: 22.699073198411195 \n",
      " unique values: 72\n",
      "investor_state_code \n",
      " NA values: 16809 \n",
      " % of empty: 31.793077359561188 \n",
      " unique values: 50\n",
      "investor_region \n",
      " NA values: 2 \n",
      " % of empty: 0.003782863627766219 \n",
      " unique values: 585\n",
      "investor_city \n",
      " NA values: 12480 \n",
      " % of empty: 23.605069037261206 \n",
      " unique values: 990\n",
      "funding_round_type \n",
      " NA values: 3 \n",
      " % of empty: 0.005674295441649328 \n",
      " unique values: 9\n",
      "funded_at \n",
      " NA values: 3 \n",
      " % of empty: 0.005674295441649328 \n",
      " unique values: 2808\n",
      "raised_amount_usd \n",
      " NA values: 3599 \n",
      " % of empty: 6.807263098165311 \n",
      " unique values: 1458\n"
     ]
    }
   ],
   "source": [
    "# This function takes in the name of the csv you want to analyze, the files encoding, a list of columns \n",
    "# to include in the analysis and a \"long\" flag that if True will print out full frequency table. If false\n",
    "# only summary statistics will be returned such as the number and share of NaN values and the number of unique\n",
    "# values. No return value.\n",
    "def get_value_frequency_and_empty_per_column(csv, size_of_chunk, file_encoding, column, long = True):\n",
    "    import pandas as pd\n",
    "    frequencies = []\n",
    "    column_list = [column]\n",
    "    empty_values = 0\n",
    "    rows = 0\n",
    "    chunk_iter = pd.read_csv(csv, chunksize = size_of_chunk, encoding = file_encoding, usecols = column_list)\n",
    "    for chunk in chunk_iter:\n",
    "        frequencies.append(chunk[column].value_counts())\n",
    "        empty_values += chunk[column].isna().sum()\n",
    "        rows += chunk.shape[0]\n",
    "    frequencies_sum = pd.concat(frequencies)\n",
    "    frequencies_sum = frequencies_sum.groupby(frequencies_sum.index).sum()\n",
    "    unique_values = frequencies_sum.shape[0]\n",
    "    if long:\n",
    "        print(frequencies_sum)\n",
    "    print(column, \"\\n NA values:\", empty_values, \"\\n % of empty:\",empty_values/rows*100, \"\\n unique values:\", unique_values)\n",
    "\n",
    "columns = pd.read_csv(\"crunchbase-investments.csv\", nrows = 1, encoding = \"ISO-8859-1\", usecols = columns_to_use).columns  \n",
    "\n",
    "for column in columns:\n",
    "    get_value_frequency_and_empty_per_column(\"crunchbase-investments.csv\", 6000, \"ISO-8859-1\",column, long = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a2dec",
   "metadata": {},
   "source": [
    "Based on the above we will exclude investor_category_code from our analysis since more than 95% of the rows don't contain this information so it wouldn't be useful for deducing anything about the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68040cc",
   "metadata": {},
   "source": [
    "And looking at the NaN values it looks like in most cases the conversion to float comes from these NaN values which for some columns are due to the fact that most of the data is from the US and most countries apart from the US don't have states within them, therefore the state code will be NaN by default. \n",
    "\n",
    "However in the case of other columns there is still a huge % of empty fields which points to the incompleteness of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07930501",
   "metadata": {},
   "source": [
    "## Data size optimization\n",
    "\n",
    "Now that we have a better understanding of the dataset let's look at how we can optimize each columns datatype for a more efficient data storage.\n",
    "\n",
    "For object (string) columns we will attempt at using the category datatype where possible and since our only numeric value has NaN values we will keep it in the form of a float as integers don't accept NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8a2f2",
   "metadata": {},
   "source": [
    "Looking at the output from our function above the columns that are suited to become category datatype are:\n",
    "- company_category_code \n",
    "- company_country_code\n",
    "- company_state_code\n",
    "- company_region\n",
    "- company_city\n",
    "- investor_country_code\n",
    "- investor_state_code\n",
    "- investor_region\n",
    "- investor_city\n",
    "- funding_round_type\n",
    "\n",
    "We also have one date column in there so let's convert that from object to date data type as well. \n",
    "\n",
    "So let's look at the optimized size after applying these changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9c08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  1.1286096572875977\n",
      "2  :  1.1288862228393555\n",
      "3  :  1.1041269302368164\n",
      "4  :  1.1190757751464844\n",
      "5  :  1.1292734146118164\n",
      "6  :  1.121281623840332\n",
      "7  :  1.1095781326293945\n",
      "8  :  1.039377212524414\n",
      "9  :  0.809748649597168\n",
      "Total memory used: 9.689957618713379\n"
     ]
    }
   ],
   "source": [
    "columns_to_use = [\"company_name\", \"company_category_code\", \"company_country_code\", \"company_state_code\", \"company_region\", \"company_city\", \"investor_name\", \"investor_country_code\", \"investor_state_code\", \"investor_region\", \"investor_city\", \"funding_round_type\", \"funded_at\", \"raised_amount_usd\" ]\n",
    "columns_dts = {\n",
    "    \"company_name\" : \"object\",\n",
    "    \"company_category_code\" : \"category\",\n",
    "    \"company_country_code\" : \"category\",\n",
    "    \"company_state_code\" : \"category\",\n",
    "    \"company_region\" : \"category\",\n",
    "    \"company_city\" : \"category\",\n",
    "    \"investor_name\" : \"object\",\n",
    "    \"investor_country_code\" : \"category\",\n",
    "    \"investor_state_code\" : \"category\",\n",
    "    \"investor_region\" : \"category\",\n",
    "    \"investor_city\" : \"category\",\n",
    "    \"funding_round_type\" : \"category\",\n",
    "    \"raised_amount_usd\" : \"float64\"\n",
    "}\n",
    "\n",
    "get_memory_usage(\"crunchbase-investments.csv\", 6000, \"ISO-8859-1\", columns = columns_to_use, data_types = columns_dts, to_date = [\"funded_at\"], detail = True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36368776",
   "metadata": {},
   "source": [
    "We have managed to cut the size of the dataset to under 10MB so we can increase our chunk size and improve the speed of all of our calculations. We could even load the whole dataset at once at we would still be within our memory limits but we will keep it to 2 chunks just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a944d6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  4.909026145935059\n",
      "2  :  4.270639419555664\n",
      "Total memory used: 9.179665565490723\n"
     ]
    }
   ],
   "source": [
    "get_memory_usage(\"crunchbase-investments.csv\", 28000, \"ISO-8859-1\", columns = columns_to_use, data_types = columns_dts, to_date = [\"funded_at\"], detail = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc855e3",
   "metadata": {},
   "source": [
    "## Analysis with SQL\n",
    "\n",
    "Now that our dataset is optimized we can start performing SQL calculations on it, we will use SQLite for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d7b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"crunchbase.db\")\n",
    "crunch_iter = pd.read_csv(\"crunchbase-investments.csv\", chunksize=28000, encoding = \"ISO-8859-1\", usecols = columns_to_use, dtype = columns_dts, parse_dates = [\"funded_at\"])\n",
    "for chunk in crunch_iter:    \n",
    "    chunk.to_sql(\"investments\", conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35c254",
   "metadata": {},
   "source": [
    "Let's check that our datatypes are correct even after conversion to SQLite datatypes. SQLite doesn't support category datatype, therefore it should be visible as \"TEXT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda6276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cid                   name       type  notnull dflt_value  pk\n",
      "0     0           company_name       TEXT        0       None   0\n",
      "1     1  company_category_code       TEXT        0       None   0\n",
      "2     2   company_country_code       TEXT        0       None   0\n",
      "3     3     company_state_code       TEXT        0       None   0\n",
      "4     4         company_region       TEXT        0       None   0\n",
      "5     5           company_city       TEXT        0       None   0\n",
      "6     6          investor_name       TEXT        0       None   0\n",
      "7     7  investor_country_code       TEXT        0       None   0\n",
      "8     8    investor_state_code       TEXT        0       None   0\n",
      "9     9        investor_region       TEXT        0       None   0\n",
      "10   10          investor_city       TEXT        0       None   0\n",
      "11   11     funding_round_type       TEXT        0       None   0\n",
      "12   12              funded_at  TIMESTAMP        0       None   0\n",
      "13   13      raised_amount_usd       REAL        0       None   0\n"
     ]
    }
   ],
   "source": [
    "datatypes = pd.read_sql(\"PRAGMA table_info(investments)\", conn)\n",
    "\n",
    "print(datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c5ac5",
   "metadata": {},
   "source": [
    "We are now going to respond to some questions regarding the dataset by using SQL to select columns and then we'll use pandas for all calculations and grouping as it is usually more efficient. For the first calculation we'll show how to leverage more SQL instead of Pandas.\n",
    "\n",
    "The questions are:\n",
    "\n",
    "- What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.\n",
    "- Which category of company attracted the most investments?\n",
    "- Which investor contributed the most money (across all startups)?\n",
    "- Which investors contributed the most money per startup?\n",
    "- Which funding round was the most popular? Which was the least popular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "60c0f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10% of companies contributed to total by: 64.56% \n",
      "Top 1% of companies contributed to total by: 25.14% \n",
      "Bottom 10% of companies contributed to total by: 0.03% \n",
      "Bottom 1% of companies contributed to total by: 0.0002%\n"
     ]
    }
   ],
   "source": [
    "data_query = \"SELECT company_name, SUM(raised_amount_usd) as raised_sum FROM investments GROUP BY company_name \"\n",
    "data = pd.read_sql(data_query, conn)\n",
    "\n",
    "# print(data.head(5))\n",
    "# We have companies that have NaN in the database and since they don't have a valid investment in the dataset\n",
    "# we will exclude them. We will also sort the data descending.\n",
    "\n",
    "data = data.dropna(subset = \"raised_sum\").sort_values(by = \"raised_sum\", ascending = False)\n",
    "# print(\"\\n\",data.head(5), \"\\n\")\n",
    "\n",
    "companies_funded_count = data.shape[0]\n",
    "total_funds = data[\"raised_sum\"].sum()\n",
    "\n",
    "top10 = round(companies_funded_count*0.1) # rounding down because we cannot split a company in half\n",
    "top1 = round(companies_funded_count*0.01)\n",
    "\n",
    "funds_top10 = data.head(top10)[\"raised_sum\"].sum()/total_funds\n",
    "funds_top1 = data.head(top1)[\"raised_sum\"].sum()/total_funds\n",
    "\n",
    "funds_bottom10 = data.tail(top10)[\"raised_sum\"].sum()/total_funds\n",
    "funds_bottom1 = data.tail(top1)[\"raised_sum\"].sum()/total_funds\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Top 10% of companies contributed to total by:\", \"{:.2%}\".format(funds_top10),\n",
    "    \"\\nTop 1% of companies contributed to total by:\", \"{:.2%}\".format(funds_top1), \n",
    "    \"\\nBottom 10% of companies contributed to total by:\", \"{:.2%}\".format(funds_bottom10), \n",
    "    \"\\nBottom 1% of companies contributed to total by:\", \"{:.4%}\".format(funds_bottom1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "91e05abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biotech category, has raised the most money.\n",
      "Money raised = 110396.423062 \n",
      "% of total = 16.19%\n"
     ]
    }
   ],
   "source": [
    "data_query = \"SELECT company_category_code, raised_amount_usd FROM investments\"\n",
    "data = pd.read_sql(data_query, conn)\n",
    "\n",
    "# grouping by company category code and performing a sum, also converting to millions USD for better readability\n",
    "\n",
    "category_sum = (data.groupby(\"company_category_code\", dropna = False).sum()/1000000).sort_values(by = \"raised_amount_usd\", ascending = False)\n",
    "\n",
    "# print(category_sum, \"\\n\")\n",
    "\n",
    "print(category_sum.head(1).first_valid_index().capitalize(), \n",
    "      \"category, has raised the most money.\\nMoney raised =\",\n",
    "    category_sum.head(1).iloc[0,0],\n",
    "     \"\\n% of total =\",\n",
    "     \"{:.2%}\".format(category_sum.head(1).iloc[0,0]/(total_funds/1000000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09b45f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kleiner perkins caufield & byers , has invested the most money.\n",
      "Money invested = 11217.826376 \n",
      "% of total = 1.65%\n"
     ]
    }
   ],
   "source": [
    "data_query = \"SELECT investor_name, raised_amount_usd FROM investments\"\n",
    "data = pd.read_sql(data_query, conn)\n",
    "\n",
    "investor_sum = (data.groupby(\"investor_name\", dropna = False).sum()/1000000).sort_values(by = \"raised_amount_usd\", ascending = False)\n",
    "\n",
    "# print(investor_sum, \"\\n\")\n",
    "\n",
    "print(investor_sum.head(1).first_valid_index().capitalize(), \n",
    "      \", has invested the most money.\\nMoney invested =\",\n",
    "    investor_sum.head(1).iloc[0,0],\n",
    "     \"\\n% of total =\",\n",
    "     \"{:.2%}\".format(investor_sum.head(1).iloc[0,0]/(total_funds/1000000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a700288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               raised_amount_usd\n",
      "investor_name                   \n",
      "Intel                     5620.0\n",
      "Comcast                   5620.0\n",
      "Time Warner               5620.0\n",
      "BrightHouse               4700.0\n",
      "Google                    3200.0 \n",
      "\n",
      "The investors that raised the most money per startup were/was: ['Intel', 'Comcast', 'Time Warner']\n"
     ]
    }
   ],
   "source": [
    "data_query = \"SELECT investor_name, company_name, raised_amount_usd FROM investments\"\n",
    "data = pd.read_sql(data_query, conn)\n",
    "\n",
    "# summing up total investments per investor per startup and getting the highest one, putting it into a list\n",
    "# this list will be later added\n",
    "\n",
    "investor_sum = (data.groupby([\"investor_name\", \"company_name\"], dropna = False).sum()/1000000).sort_values(by = \"investor_name\", ascending = False)\n",
    "investor_max = investor_sum.groupby([\"investor_name\"], dropna = False).max().sort_values(by = \"raised_amount_usd\", ascending = False)\n",
    "\n",
    "print(investor_max.head(5), \"\\n\")\n",
    "print(\"The investors that raised the most money per startup were/was:\",\n",
    "     investor_max[investor_max[\"raised_amount_usd\"] == investor_max.iloc[0,0]].index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "149a5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular funding round (with most investments) hapenned on: 2008-01-01 00:00:00 \n",
      "There wasn't one specifc round with the least amount of investements, but the smallest number of investments per round was  1 and there were 208 of them.\n"
     ]
    }
   ],
   "source": [
    "data_query = \"SELECT funded_at, raised_amount_usd FROM investments\"\n",
    "data = pd.read_sql(data_query, conn)\n",
    "\n",
    "# The popularity of a funding round will be defined as the funding round during which, the biggest amount of \n",
    "# investments happened not by the sum of funds raised.\n",
    "\n",
    "investments_count = (data.groupby(\"funded_at\").count()).sort_values(by = \"raised_amount_usd\", ascending = False)\n",
    "investments_count_not0 = investments_count[investments_count[\"raised_amount_usd\"]>0]\n",
    "investments_count_smallest = investments_count[investments_count[\"raised_amount_usd\"] == investments_count_not0.iloc[-1,0]]\n",
    "\n",
    "# print(investments_count)\n",
    "# print(investments_count_smallest)\n",
    "\n",
    "print(\"The most popular funding round (with most investments) hapenned on:\",\n",
    "     investments_count.first_valid_index(),\n",
    "     \"\\nThere wasn't one specifc round with the least amount of investements, but the smallest number of investments per round was \",\n",
    "     investments_count_not0.iloc[-1,0],\n",
    "     \"and there were\",\n",
    "     investments_count_smallest.shape[0],\n",
    "     \"of them.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
