{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a158848f",
   "metadata": {},
   "source": [
    "# 👩🏼‍💻 Hacker News Posts Analysis\n",
    "According to wikipedia Hacker News is a \" social news website focusing on computer science and entrepreneurship. It is run by the investment fund and startup incubator Y Combinator. In general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity.\"\"\n",
    "\n",
    "In this analysis we are going to look at the stories(posts) posted on the website in 2022 with a particular interest on Ask HN and Show HN posts. The main outcome of this analysis will be to see what is the best time to submit these specific posts on the platform to get the biggest engagement (number of comments).\n",
    "\n",
    "This use case can be replicated for any other type of a post or website that provides their data publicly.\n",
    "\n",
    "This project is inspired by Dataquest's project that analyses data from 2016. Since I wanted to make the analsys more relevant and bring something new to it I've decided to get the latest data from 2022. This data is now widely available in the form of a public dataset in Google's Analytics Hub as described in [this](https://github.com/minimaxir/get-all-hacker-news-submissions-comments) github post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16196c",
   "metadata": {},
   "source": [
    "## Acquiring the data\n",
    "\n",
    "**Step 1**\n",
    "Open BigQuery and access the \"bigquery-public-data.hacker_news.full\" dataset from the Analytics Hub.\n",
    "\n",
    "**Step 2**\n",
    "Write a query to extract data from 2022. The table contains both comments and stories, therefore you need to create a temporary table or a subquery to calculate the number of comments per each article. A better scenario would be to have these two separate and connected by an identifier. Based on my investigation it looks like comments have the ID of the article in the parents column so that is what I used to join these two datapoints.\n",
    "\n",
    "-- A: Create a subquery that will get the number of comments per parent(article)\n",
    "\n",
    "WITH\n",
    "  comments AS (\n",
    "    SELECT parent, count(id) as no_of_comments\n",
    "    FROM `bigquery-public-data.hacker_news.full`\n",
    "    WHERE type = \"comment\" AND EXTRACT(YEAR FROM timestamp) >= 2022\n",
    "    GROUP BY parent\n",
    "  )\n",
    "  \n",
    "-- B: Get a list of parents(articles) and join them with the number of comments calculated in step 1\n",
    "\n",
    "SELECT title, timestamp, id, comments.no_of_comments\n",
    "FROM `bigquery-public-data.hacker_news.full` JOIN comments ON comments.parent = id\n",
    "WHERE type = \"story\" AND EXTRACT(YEAR FROM timestamp) = 2022\n",
    "\n",
    "**Step 3**\n",
    "Export the data. There are multiple options to do this, however, this file was too big to be exported as csv and therefore I decided to go for the jsonl option. We will explore how to load a json in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d43901a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'timestamp', 'id', 'no_of_comments']\n",
      "['Coinbase CEO tweets how they came up with Super Bowl ad, lied about it', '2022-02-21 21:41:29 UTC', '30420990', '51']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSONL file\n",
    "opened_file = open('HN_2022_JSON.json')\n",
    "# Converting a JSONL file to a list of dictionaries\n",
    "read_file = list(opened_file)\n",
    "\n",
    "opened_file.close()\n",
    "\n",
    "# Getting the header row by picking first dictionary and accessing its keys\n",
    "row = json.loads(read_file[:1][0])\n",
    "header = list(row.keys())\n",
    "\n",
    "# Iterating over each row and storing values from each dictionary as a separate list in the list of lists\n",
    "hn = []\n",
    "for json_str in read_file:\n",
    "    row = json.loads(json_str)\n",
    "    row = list(row.values())\n",
    "    hn.append(row)\n",
    "\n",
    "# Result is a list of lists (essentially a table) containing following columns \n",
    "print(header)\n",
    "\n",
    "# Example of one row of data\n",
    "print(hn[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba01b96",
   "metadata": {},
   "source": [
    "## Analyzing the data\n",
    "In this part we are going to analyze our dataset. In order to get to our objective which I'll repeat is: \"what is the best time to submit these specific posts on the platform to get the biggest engagement (number of comments)\" we will do the following steps:\n",
    "\n",
    "1. Filter out all posts that start with \"Ask HN\" or \"Show HN\"\n",
    "2. Calculate the number of posts and comments that are created at every hour\n",
    "3. Calculate what is the average number of comments per post per hour and sort them from high to low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d65dd0",
   "metadata": {},
   "source": [
    "**1.Filter out all posts that contain \"Ask HN\" or \"Show HN\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb23954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a dataset that contains a column of strings which we want to filter based on what is in the\n",
    "# beginning of the string. Then it takes in the substring that should be at the start and the column position. Finally,\n",
    "# there are two default arguments capitalise and inlcude. If capitalise is set to False we will disregard \n",
    "# capitalisation of the text in case it is true we will take into consideration whether the substring contains \n",
    "# uppercase or lowercase letters. If include is set to true it means we want to filter strings that contain the \n",
    "# selected string, if false give then it will remove all of the entries containing set substring and return that \n",
    "# dataset.\n",
    "# This function returns a filtered dataset containing only rows that start with the specified substring in the given \n",
    "# column.\n",
    "\n",
    "def filter_start_string_column(dataset, filter_string, column, capitalize = False, include = True):\n",
    "    filtered_data = []\n",
    "    if capitalize == False:\n",
    "        filter_string = filter_string.lower()\n",
    "        for row in dataset:\n",
    "            lowered = row[column].lower()\n",
    "            if lowered.startswith(filter_string) == include:\n",
    "                filtered_data.append(row)\n",
    "    else:\n",
    "        for row in dataset:\n",
    "            if row[column].startswith(filter_string) == include:\n",
    "                filtered_data.append(row)\n",
    "    return filtered_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ec3d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_posts = filter_start_string_column(hn,'Ask HN',0)\n",
    "show_posts = filter_start_string_column(hn,'Show HN',0)\n",
    "\n",
    "# Filtering out all remaining posts by doing inverse filtering using the \"include\" argument\n",
    "other_posts = filter_start_string_column(hn,'Ask HN',0, include = False)\n",
    "other_posts = filter_start_string_column(other_posts,'Show HN',0, include = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73d7d214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11887\n",
      "4705\n",
      "101560\n",
      "118152\n"
     ]
    }
   ],
   "source": [
    "# Printing lengths of all lists for checking that the above function processes the whole input\n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))\n",
    "\n",
    "print(len(hn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1810a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_per_column(dataset, column):\n",
    "    sum = 0\n",
    "    for row in dataset:\n",
    "        sum += int(row[column])\n",
    "    return sum/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bcb8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the overal average of comments per post type\n",
    "avg_ask_comments = calculate_average_per_column(ask_posts,3)\n",
    "avg_show_comments = calculate_average_per_column(show_posts,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34a7318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.515773534112897\n",
      "6.007226354941552\n"
     ]
    }
   ],
   "source": [
    "print(avg_ask_comments)\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56ca78",
   "metadata": {},
   "source": [
    "The above averages show that Hacker News is a very active social network that can provide answers when you ask since the average number of comments for ask comments is almost double the amount for Show HN posts.\n",
    "\n",
    "**2. Calculate the number of posts and comments that are created at every hour**\n",
    "Now we're going to look at the number of posts and their comments that are submitted at every hour. This can help us understand which times are the less busy which can lead to a better visbility of our posts.\n",
    "\n",
    "In order to do that we need to treat the timestamp data we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afe88690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-17 11:58:58 UTC\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Testing out the strptime formatting\n",
    "date_hour = datetime.strptime(ask_posts[0][1],\"%Y-%m-%d %H:%M:%S %Z\") #'2022-06-08 01:26:10 UTC'\n",
    "print(ask_posts[0][1])\n",
    "print(date_hour.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8f5eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': 466, '15': 790, '16': 792, '22': 497, '19': 653, '12': 533, '20': 617, '9': 395, '13': 589, '18': 695, '8': 361, '7': 316, '14': 721, '17': 770, '4': 351, '21': 587, '3': 276, '10': 433, '5': 295, '1': 346, '2': 335, '23': 404, '6': 287, '0': 378}\n",
      "{'11': 7201, '15': 16784, '16': 12212, '22': 4640, '19': 5727, '12': 7645, '20': 5233, '9': 4546, '13': 7526, '18': 6525, '8': 4873, '7': 3429, '14': 8613, '17': 7626, '4': 3347, '21': 5112, '3': 2414, '10': 5504, '5': 2720, '1': 3523, '2': 3540, '23': 2668, '6': 2842, '0': 2638}\n",
      "{'4': 76, '6': 106, '18': 306, '8': 120, '15': 379, '13': 316, '14': 357, '9': 142, '16': 388, '12': 274, '22': 161, '19': 265, '21': 208, '2': 83, '10': 174, '17': 340, '11': 211, '0': 87, '7': 124, '20': 220, '3': 81, '1': 85, '23': 122, '5': 80}\n",
      "{'4': 669, '6': 613, '18': 2112, '8': 691, '15': 2243, '13': 2561, '14': 2112, '9': 1123, '16': 2394, '12': 1867, '22': 791, '19': 1530, '21': 861, '2': 387, '10': 895, '17': 1980, '11': 1014, '0': 418, '7': 525, '20': 968, '3': 455, '1': 504, '23': 1304, '5': 247}\n"
     ]
    }
   ],
   "source": [
    "number_of_ask_posts = {}\n",
    "number_of_ask_comments = {}\n",
    "number_of_show_posts = {}\n",
    "number_of_show_comments = {}\n",
    "\n",
    "# Iterating over each row in the lists and extracting the hour when they were posted\n",
    "# With each passing we augment the number of posts and we also addition the number of comments to get a \n",
    "# dictionary that will have hours of the day as keys and number of posts and comments as values\n",
    "for row in ask_posts:\n",
    "    post_date = datetime.strptime(row[1],\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    post_hour = str(post_date.hour)\n",
    "    if post_hour in number_of_ask_posts:\n",
    "        number_of_ask_posts[post_hour] += 1\n",
    "        number_of_ask_comments[post_hour] += int(row[3])\n",
    "    else:\n",
    "        number_of_ask_posts[post_hour] = 1\n",
    "        number_of_ask_comments[post_hour] = int(row[3])\n",
    "\n",
    "print(number_of_ask_posts)\n",
    "print(number_of_ask_comments)\n",
    "\n",
    "for row in show_posts:\n",
    "    post_date = datetime.strptime(row[1],\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    post_hour = str(post_date.hour)\n",
    "    if post_hour in number_of_show_posts:\n",
    "        number_of_show_posts[post_hour] += 1\n",
    "        number_of_show_comments[post_hour] += int(row[3])\n",
    "    else:\n",
    "        number_of_show_posts[post_hour] = 1\n",
    "        number_of_show_comments[post_hour] = int(row[3])\n",
    "\n",
    "print(number_of_show_posts)\n",
    "print(number_of_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27e585",
   "metadata": {},
   "source": [
    "**3. Calculate what is the average number of comments per post per hour and sort them from high to low**\n",
    "As our final step we are going to take the two dictionaries created previously and we'll get the average number of comments per each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c5e10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_posts_per_hour = []\n",
    "show_posts_per_hour = []\n",
    "\n",
    "for key in number_of_ask_comments:\n",
    "    ask_posts_per_hour.append([key, number_of_ask_comments[key]/number_of_ask_posts[key]])\n",
    "    \n",
    "for key in number_of_show_comments:\n",
    "    show_posts_per_hour.append([key, number_of_show_comments[key]/number_of_show_posts[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed6e650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11', 15.452789699570815], ['15', 21.245569620253164], ['16', 15.419191919191919], ['22', 9.336016096579478], ['19', 8.770290964777947], ['12', 14.343339587242026], ['20', 8.481361426256077], ['9', 11.50886075949367], ['13', 12.777589134125636], ['18', 9.388489208633093], ['8', 13.498614958448753], ['7', 10.85126582278481], ['14', 11.945908460471568], ['17', 9.903896103896104], ['4', 9.535612535612536], ['21', 8.708688245315162], ['3', 8.746376811594203], ['10', 12.711316397228638], ['5', 9.220338983050848], ['1', 10.18208092485549], ['2', 10.567164179104477], ['23', 6.603960396039604], ['6', 9.902439024390244], ['0', 6.978835978835979]]\n",
      "[['4', 8.802631578947368], ['6', 5.783018867924528], ['18', 6.901960784313726], ['8', 5.758333333333334], ['15', 5.91820580474934], ['13', 8.104430379746836], ['14', 5.915966386554622], ['9', 7.908450704225352], ['16', 6.170103092783505], ['12', 6.813868613138686], ['22', 4.913043478260869], ['19', 5.773584905660377], ['21', 4.139423076923077], ['2', 4.662650602409639], ['10', 5.14367816091954], ['17', 5.823529411764706], ['11', 4.805687203791469], ['0', 4.804597701149425], ['7', 4.233870967741935], ['20', 4.4], ['3', 5.617283950617284], ['1', 5.929411764705883], ['23', 10.688524590163935], ['5', 3.0875]]\n"
     ]
    }
   ],
   "source": [
    "# Average number of comments per hour of ask posts\n",
    "print(ask_posts_per_hour)\n",
    "print(show_posts_per_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3c2aa",
   "metadata": {},
   "source": [
    "Now we'll just format and sort the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc62e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['15', 21.245569620253164], ['11', 15.452789699570815], ['16', 15.419191919191919], ['12', 14.343339587242026], ['8', 13.498614958448753]]\n",
      "[['23', 10.688524590163935], ['4', 8.802631578947368], ['13', 8.104430379746836], ['9', 7.908450704225352], ['18', 6.901960784313726]]\n"
     ]
    }
   ],
   "source": [
    "# Sorting the hours by the average number of comments and keeping only top 5\n",
    "from operator import itemgetter \n",
    "\n",
    "sorted_ask_list = sorted(ask_posts_per_hour, key=itemgetter(1), reverse=True)\n",
    "print(sorted_ask_list[:5])\n",
    "\n",
    "sorted_show_list = sorted(show_posts_per_hour, key=itemgetter(1), reverse=True)\n",
    "print(sorted_show_list[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bdea202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 15:00 the average number of comments was: 21.25\n",
      "At 11:00 the average number of comments was: 15.45\n",
      "At 16:00 the average number of comments was: 15.42\n",
      "At 12:00 the average number of comments was: 14.34\n",
      "At 08:00 the average number of comments was: 13.50\n"
     ]
    }
   ],
   "source": [
    "for row in sorted_ask_list[:5]:\n",
    "    text = \"At {:02d}:00 the average number of comments was: {:.2f}\".format(int(row[0]), row[1])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733ea7f",
   "metadata": {},
   "source": [
    "Based on the above, it looks like if you want to get more answers to your \"Ask HN\" posts they should be posted at 3pm UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f8ca1efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 23:00 the average number of comments was: 10.69\n",
      "At 04:00 the average number of comments was: 8.80\n",
      "At 13:00 the average number of comments was: 8.10\n",
      "At 09:00 the average number of comments was: 7.91\n",
      "At 18:00 the average number of comments was: 6.90\n"
     ]
    }
   ],
   "source": [
    "for row in sorted_show_list[:5]:\n",
    "    text = \"At {:02d}:00 the average number of comments was: {:.2f}\".format(int(row[0]), row[1])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589d4db",
   "metadata": {},
   "source": [
    "Surprisingly, for \"Show HN\" posts the trend is completely different. The best time to post for best engagement is at 11pm UTC. Now this can have many explications that we won't be able to get from the dataset but my main hypothesis is:\n",
    "\n",
    "**The distribution of users time zone.** The data is collected using UTC which is the universal time based on Greenwhich time meaning that it is very different to US time zones for example. Hacker News is based in [New York](https://www.crunchbase.com/organization/the-hacker-news) which might suggest(or it might not) that a large set of users are from US and therefore 23:00 UTC will be a much more reasonable evening time in most of US. \n",
    "\n",
    "Let's convert the time zone to EDT just to show how one might look up at which time he should publish their posts in their local time zone. \n",
    "EDT (Eastern Daylight time) is 4 hours behind UTC, also formerly known as GMT. For easy conversion you can visit [this](https://savvytime.com/converter/utc-to-est) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a95d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts given time from a selected time zone to another one. The source of the abbreviations used\n",
    "# comes from https://www.timeanddate.com/time/zones/.\n",
    "\n",
    "def convert_time(hour, from_zone, to_zone, minute = 0, file = \"UTC.csv\"):\n",
    "    from csv import reader\n",
    "    with open(file) as file:\n",
    "        read_file = reader(file)\n",
    "        time_zones_UTC = list(read_file)\n",
    "        time_zones_UTC = time_zones_UTC[1:] # removing header\n",
    "        \n",
    "    # In case the user uploads a file with lower case timezones\n",
    "    from_zone = from_zone.upper()\n",
    "    to_zone = to_zone.upper()\n",
    "        \n",
    "    UTC_dict = {}\n",
    "    for row in time_zones_UTC:\n",
    "        zone = row[0].upper()\n",
    "        hours = row[1]\n",
    "        minutes = row[2]\n",
    "        UTC_dict[zone] = [hours, minutes]\n",
    "    \n",
    "    # Checks input for validity i.e. the zone is on the list\n",
    "    if (from_zone not in UTC_dict) or (to_zone not in UTC_dict):\n",
    "        return(\"Sorry this time zone isn't on the list.\")\n",
    "\n",
    "    minute_UTC = int(minute) - int(UTC_dict[from_zone][1]) + int(UTC_dict[to_zone][1])\n",
    "\n",
    "    overflow_hour = minute_UTC/60\n",
    "\n",
    "    if overflow_hour >= 1:\n",
    "        # Using a modulo in case there is an overflow to another day\n",
    "        hour_UTC = (hour + floor(overflow_hour) - int(UTC_dict[from_zone][0]) + int(UTC_dict[to_zone][0]))%24\n",
    "\n",
    "    else:\n",
    "        hour_UTC = (hour - int(UTC_dict[from_zone][0]) + int(UTC_dict[to_zone][0]))%24\n",
    "    return('{:02d}:{:02d}'.format(hour_UTC, minute_UTC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f6fc1",
   "metadata": {},
   "source": [
    "Now let's try the new function on the two lists we've created previously. As mentioned above we are converting UTC to EDT.\n",
    "\n",
    "Ask posts first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b0fe6918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 11:00 the average number of comments was: 21.25\n",
      "At 07:00 the average number of comments was: 15.45\n",
      "At 12:00 the average number of comments was: 15.42\n",
      "At 08:00 the average number of comments was: 14.34\n",
      "At 04:00 the average number of comments was: 13.50\n"
     ]
    }
   ],
   "source": [
    "for row in sorted_ask_list[:5]:\n",
    "    print(\"At\", convert_time(int(row[0]), \"UTC\", \"EDT\"), \"the average number of comments was: {:.2f}\".format(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275e6d3",
   "metadata": {},
   "source": [
    "Finally, let's look at show posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a148028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 19:00 the average number of comments was: 10.69\n",
      "At 00:00 the average number of comments was: 8.80\n",
      "At 09:00 the average number of comments was: 8.10\n",
      "At 05:00 the average number of comments was: 7.91\n",
      "At 14:00 the average number of comments was: 6.90\n"
     ]
    }
   ],
   "source": [
    "for row in sorted_show_list[:5]:\n",
    "    print(\"At\", convert_time(int(row[0]), \"UTC\", \"EDT\"), \"the average number of comments was: {:.2f}\".format(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725cf9c",
   "metadata": {},
   "source": [
    "The above shows us that if you're living in the US, the best time to post ask posts would be in the morning hours whereas for show posts it is distributed more evenly during the day. This could mean that comments on ask posts are coming from users in timezones that are more similar. On the other hand the more even distribution of show points could mean that people either read the posts throughtout the day or each of the peaks corresponds to a different timezone. An analysis of the user base could reveal that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
